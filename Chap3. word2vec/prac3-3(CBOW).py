# CBOW 모델은 word2vec에서 자주 사용되는 신경망 모델 중 하나
# CBOW 모델은 맥락(주변 단어)으로부터 타깃(찾을 중앙 단어)을 추측하는 용도의 신경망
# 맥락(주변 단어)을 입력하면 CBOW 모델이 추론할 수 있도록 맥락을 원-핫 벡터로 변환하는 과정 필요

# 말뭉치가 'you say hello and I say goodbye.'로 단어 수가 7개라 가정

# 맥락으로 사용할 단어가 N개라면 입력층이 N개 벡터가 되며, 입력층이 완전연결계층을 통해 은닉층으로 전달
# 은닉층에서 다시 완전연결계층을 거져 출력층으로 1개 벡터를 전달

# 은닉층의 경우 입력층(벡터)이 여러 개일 경우 전체를 평균하여 계산 (덧셈과 곱셈 계층 필요)
# 출력층의 경우 뉴런은 말뭉치의 각 단어에 대응하는 7개이고, 뉴런의 값은 각 단어의 점수를 나타내며 점수가 높을 수록 해당 단어의 출현 확률이 증가

# 입력층에서 은닉층으로의 변환하는 가중치의 각 행이 해당 단어의 단어의 분산 표현

# 2개 벡터 입력값 -> 각각의 입력값에 대해 은닉층 계산을 위한 행렬곱(MatMul) 계층 2개 -> 
# 2개 벡터 덧셈 계층 -> 곱셈 계층(0.5) -> 출력값 계산을 위한 행렬곱(MatMul) 계층 -> 출력층 순으로 진행

# 이러한 작업을 수행하는 신경망 계층을 구현

import os, sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
import numpy as np
from common.layers import MatMul

# 맥락 데이터 1 (0번 인덱스 단어 'you') 벡터를 1번째 입력값 c0으로 저장
c0 = np.array([1,0,0,0,0,0,0])
# 맥락 데이터 2 (2번 인덱스 단어 'hello') 벡터를 2번째 입력값 c1으로 저장
c1 = np.array([0,0,1,0,0,0,0])

# 입력층->은닉층을 계산하는 7×3 형상의 가중치 초기화
W_in = np.random.randn(7, 3)
# 은닉층->출력층을 계산하는 3×7 형상의 가중치 초기화
W_out = np.random.randn(3, 7)

# 1번째 입력값 c0를 행렬곱하는 MatMul 계층에 가중치 입력
in_layer0 = MatMul(W_in)
# 2번째 입력값 c1을 행렬곱하는 MatMul 계층에 가중치 입력 
in_layer1 = MatMul(W_in)
# 은닉층을 행렬곱하는 MatMul 계층에 가중치 입력
out_layer = MatMul(W_out)

# 1번째 입력값 c0에 대하여 행렬곱 순전파 계산 후 결과값을 1번째 은닉층 h0으로 저장
h0 = in_layer0.forward(c0)
# 2번째 입력값 c1에 대하여 행렬곱 순전파 계산 후 결과값을 2번째 은닉층 h1으로 저장
h1 = in_layer1.forward(c1)
# 1번째 은닉층과 2번째 은닉층의 평균을 계산하여 은닉층 h로 통합
h = 0.5 * (h0 + h1)
# 은닉층 h에 대하여 행렬곱 순전파 계산 후 결과값을 출력층 s로 저장
s = out_layer.forward(h)

# 출력층 s 출력
print(s)


# 위 계산에서 출력층 s(각 단어 별 점수벡터)에 소프트맥스 함수를 적용하면 각 단어별 출현 확률을 얻을 수 있음 
# 이러한 CBOW 모델을 학습시키면 가중치 W들에 단어의 출현 패턴을 파악한 벡터가 학습되어 저장되고 이러한 단어 출현 패턴은 우리의 직관에 부합하는 경우가 많음
# 이러한 학습 과정에는 소프트맥스 함수와 교차 엔트로피 오차를 사용 가능
# 소프트맥스 함수를 통해 점수를 확률로 변환 후 해당 확률과 정답 레이블로부터 교차 엔트로피 오차를 구하여 손실값을 얻어 학습 진행
# 따라서 위 모델의 출력층 뒤에 소프트맥스 함수와 교차 엔트로피 오차 계층을 추가하면 손실 값을 구할 수 있음

# word2vec 모델의 가중치는 위와 같이 입력층 가중치(W_in)와 출력층 가중치(W_out) 2개로 구분
# 두 개의 가중치 모두 단어의 분산 표현이 저장되어 있으나, 주로 입력층 가중치(W_in)를 최종 단어의 분산 표현으로 이용