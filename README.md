# DeepLearningStudy2

딥러닝 공부하면서 실습한 파일 정리하는 곳

Deep Learning from Scratch 밑바닥부터 시작하는 딥러닝 2(사이토 고키, 한빛 미디어) 책 기반

<a href="https://github.com/hcm1206/DeepLearningStudy">DeepLearningStudy</a> 후속 공부

******
  
> 2022-12-26 딥러닝 공부 시즌 2 개시  
  
  
> 책에서 제공하는 코드들 중 조건에 따라 아래와 같이 저장  
> - 클래스 및 기능 부분 : common 폴더  
> - 학습용 데이터셋 : dataset 폴더  
> - 실습 코드 중 다른 코드에서 추가적으로 불러서 사용하는 코드 : files 폴더  
  
  
> 책에서는 상위 디렉토리 접근을 위하여 ```sys.path.append('..')``` 코드를 사용하였으나 작동하지 않는 관계로(아나콘다 실행 환경과 충돌하는 것으로 추정)  
> 해당 코드를 ```sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))```로 변경  
   
******
  
## 최근 진행 내용

### 2023-04-08
#### Chapter4
4.2 ~ 4.2.7(p.157~p.175)
- 어휘량이 많으면 은닉층 뉴런과 출력 측의 가중치 행렬과의 행렬곱 계산과 확률을 추출하는 Softmax 계층에서의 계산에 부하가 걸림  
- 네거티브 샘플링(Negative Sampling) 기법의 핵심 아이디어는 다중 분류에서 이진 분류로 바꾸는 것  
- 다중 분류는 여러 개의 선택지 중 하나를 고르는 것이라면, 이진 분류는 이 선택지가 참인지 거짓인지 고르는 것  
- 점수에 시그모이드 함수를 적용하여 확률로 변환하고, 교차 엔트로피 오차를 사용하여 손실을 구하는 방법으로 이진 분류 문제 해결 가능  
- EmbeddingDot 계층 구현 : 예측한 타깃값의 단어 벡터를 구하여(Embedding) 저장한 가중치 Wout와 은닉층(h)의 행렬 내적(Dot) 계산을 통합한 계층  
- 네거티브 샘플링 : 정답(긍정적 예)인 경우를 제외한 오답(부정적 예) 데이터 중 일부를 뽑아서(샘플링) 신경망 학습에 사용하는 것  
- word2vec에서의 네거티브 샘플링은 말뭉치의 확률분포에 0.75를 제곱한 값을 확률분포로 하여 부정적 예 데이터 샘플링을 진행  
- NegativeSamplingLoss 계층 구현 : 네거티브 샘플링으로 추출한 부정적 예와 긍정적 예를 은닉층과의 내적 계산 및 손실 값 계산을 통해 신경망 학습을 위한 기울기 계산  

### 2023-04-02
#### Chapter4
4 ~ 4.1.2(p.149~p.157)
- 말뭉치에 포함된 어휘량이 많은 복잡한 CBOW 모델은 계산량이 크기 때문에 성능 문제가 발생  
- 성능 개선을 위하여 Embedding 계층과 네거티브 샘플링 계층을 도입하여 실용적인 word2vec 완성 가능  
- 어휘 수가 많아지면 입력층의 원-핫 벡터와 은닉층 간의 행렬곱을 위한 가중치 계산이 복잡해짐  
- 입력층에서 은닉층으로의 계산은 결과적으로 단순히 가중치 행렬의 특정 행을 추출하는 것에 불과하므로 복잡한 행렬곱 계산을 할 필요가 없음  
- Embedding 계층 구현 : 추출하고자 하는 단어 인덱스를 입력하여 가중치 행렬에서 해당 인덱스의 단어 밀집벡터를 출력  

