# DeepLearningStudy2

딥러닝 공부하면서 실습한 파일 정리하는 곳

Deep Learning from Scratch 밑바닥부터 시작하는 딥러닝 2(사이토 고키, 한빛 미디어) 책 기반

<a href="https://github.com/hcm1206/DeepLearningStudy">DeepLearningStudy</a> 후속 공부

******
  
> 2022-12-26 딥러닝 공부 시즌 2 개시  
  
  
> 책에서 제공하는 코드들 중 조건에 따라 아래와 같이 저장  
> - 클래스 및 기능 부분 : common 폴더  
> - 학습용 데이터셋 : dataset 폴더  
> - 실습 코드 중 다른 코드에서 추가적으로 불러서 사용하는 코드 : files 폴더  
  
  
> 책에서는 상위 디렉토리 접근을 위하여 ```sys.path.append('..')``` 코드를 사용하였으나 작동하지 않는 관계로(아나콘다 실행 환경과 충돌하는 것으로 추정)  
> 해당 코드를 ```sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))```로 변경  
   
******
  
## 최근 진행 내용

### 2023-04-02
#### Chapter4
4 ~ 4.1.2(p.149~p.157)
- 말뭉치에 포함된 어휘량이 많은 복잡한 CBOW 모델은 계산량이 크기 때문에 성능 문제가 발생
- 성능 개선을 위하여 Embedding 계층과 네거티브 샘플링 계층을 도입하여 실용적인 word2vec 완성 가능
- 어휘 수가 많아지면 입력층의 원-핫 벡터와 은닉층 간의 행렬곱을 위한 가중치 계산이 복잡해짐
- 입력층에서 은닉층으로의 계산은 결과적으로 단순히 가중치 행렬의 특정 행을 추출하는 것에 불과하므로 복잡한 행렬곱 계산을 할 필요가 없음
- Embedding 계층 구현 : 추출하고자 하는 단어 인덱스를 입력하여 가중치 행렬에서 해당 인덱스의 단어 밀집벡터를 출력

### 2023-03-27  
#### Chapter3  
3.4 ~ 3.6(p.135~p.147)
- 간단한 CBOW 신경망 모델 구현  
- CBOW 모델의 순전파 계산과 역전파 계산 구현   
- 기존에 구현했던 학습용 모듈 코드들과 CBOW 모델을 결합하여 CBOW 학습 코드 구현   
- 확률 표기법 : A와 B가 동시에 일어날 동시확률은 P(A,B)로, B가 일어난 후 A가 일어날 사후 확률은 P(A|B)로 표기
- CBOW 모델에서 구하는 것은 맥락 단어가 나타났을 때 타깃함수가 나타날 확률이므로 이를 이용하여 손실함수를 간결하게 표현 가능
- skip-gram 모델 : 주변 맥락 단어로부터 중앙 타깃 단어를 추측하는 CBOW 모델과 반대로 중앙 타깃 단어로부터 주변 맥락 단어를 추측하는 것
- skip-gram 모델에서는 입력층이 하나이고 출력층이 윈도우 크기(맥락의 수)만큼 존재
- skip-gram 모델은 각 맥락별 손실함수를 모두 구해야 하므로 학습속도가 느리지만 단어 표현 정밀도 면에서 CBOW 모델보다 더 좋은 결과를 도출
- skip-gram 모델의 기본적인 매커니즘은 CBOW 모델과 크게 다르지 않으므로 구현하기 어렵지 않음
- 통계 기반 기법과 추론 기반 기법은 단어 분산 표현과 정밀도 면에서는 성능 차이가 크지 않음
- 매개변수를 새로 추가할 때는 기존 학습 모델을 이용할 수 있는 추론 기반 기법이 더 효율적
- 추론 기반 기법과 통계 기반 기법은 서로 관련되어 있으며 GloVe 기법 등에서는 두 기법을 융합하기도 함

### 2023-03-24 
#### Chapter3  
3.3 ~ 3.3.2(p.131~p.134)
- word2vec 신경망에서 입력값은 찾을 단어의 주변 단어인 맥락, 출력값은 찾을 단어인 타깃  
- 말뭉치의 단어 ID 행렬에서 맥락값 행렬과 그에 대응되는 타깃값 행렬을 추출하는 함수 구현   
- CBOW 모델에서 사용하기 위해 단어 ID를 원-핫 형태로 변환하는 함수 구현  
  
### 2023-03-22  
#### Chapter3  
3 ~ 3.2.3(p.113~p.130)
- 추론 기반 기법 : 단어의 출현 패턴을 학습하여 말뭉치 중 가장 적절한 단어를 추론하는 자연어 처리 기법  
- 추론 기반 신경망에서는 입력층에 말뭉치의 원-핫 벡터를 입력한 후 행렬곱하는 완전연결계층으로 은닉층을 구현   
- CBOW 모델 : word2vec에서 제안하는 맥락(주변 단어)으로부터 타깃(찾을 중앙 단어)을 추측하는 용도의 신경망    
- CBOW 모델에서는 맥락에 따라 입력층 벡터의 개수가 달라지며 은닉층에서 벡터들의 평균을 구하는 과정 필요  
- CBOW 모델의 가중치의 각 행이 해당 단어의 분산 표현을 나타냄  
- CBOW 모델의 추론 처리 예시 구현  
- CBOW 모델의 출력 부분에 소프트맥스 함수와 교차 엔트로피 오차를 적용하면 손실 값을 구하여 모델의 신경망 학습이 가능  
- CBOW 모델에서는 주로 입력 부분 가중치를 단어의 분산 표현으로 사용   
  
### 2023-03-21   
#### Chapter2  
2.4 ~ 2.5(p.97~p.112)
- 점별 상호정보량(PMI) : 고빈도 단어와 관련 없는 단어 간의 유사도가 높게 나오는 현상을 줄인 척도  
- 양의 상호정보량(PPMI) : PMI에서 두 단어의 동시발생 횟수가 0이면 음의 무한대가 나타나는 문제를 수정한 척도, 실구현에 가장 많이 사용  
- 동시발생 행렬을 PPMI 행렬로 변환하는 함수 구현
- 차원 감소 : 중요한 정보의 손실을 최소화하며 벡터의 차원을 줄이는 방법
- 특잇값 분해(SVD) : 임의의 행렬을 세 행렬의 곱으로 분해하여 희소 행렬을 밀집 벡터로 바꾸는 차원 감소 방법 중 하나
- SVD를 이용한 차원 감소는 numpy의 linalg.svd() 함수(저속) 또는 sklearn의 randomized_svd() 함수(고속)를 이용하여 구현 가능
- PTB 데이터셋 : 자연어 처리 기법 벤치마킹용으로 널리 사용되는 적당한 크기의 말뭉치 데이터셋
- PTB 데이터셋에 통계 기반 기법을 적용하고 SVD를 통해 밀집벡터로 변환하는 과정 구현 
- 자연어 처리를 위한 다양한 전처리 방법 정리
  

  


  
  

  
  





